<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-11-29T17:25:01-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">austin hartman</title><author><name>Austin Hartman</name></author><entry><title type="html">Are doublets detected via similarity to artificial doublets valid?</title><link href="http://localhost:4000/2020/11/29/validating-artifially-detected-doublets.html" rel="alternate" type="text/html" title="Are doublets detected via similarity to artificial doublets valid?" /><published>2020-11-29T00:00:00-08:00</published><updated>2020-11-29T00:00:00-08:00</updated><id>http://localhost:4000/2020/11/29/validating-artifially-detected-doublets</id><content type="html" xml:base="http://localhost:4000/2020/11/29/validating-artifially-detected-doublets.html">&lt;p&gt;In this post I’m going to briefly explore the validity of a specific 
type of doublet detection method, by assessing the validity of putative 
doublets by a mode not explored in the method’s &lt;a href=&quot;https://doi.org/10.1016/j.cels.2018.11.005&quot;&gt;paper&lt;/a&gt;. The method 
of doublet detection I am curious about are doublets detected based 
on similarity to artificially generated doublets. Multiple variants of
this method have been published. For now, I am going to focus on one 
method called Scrublet. (&lt;a href=&quot;https://doi.org/10.1016/j.cels.2018.11.005&quot;&gt;Scrublet&lt;/a&gt;, &lt;a href=&quot;https://doi.org/10.1016/j.cels.2019.03.003&quot;&gt;DoubletFinder&lt;/a&gt;)&lt;/p&gt;

&lt;h3 id=&quot;detecting-doublets-by-doublet-simulation&quot;&gt;Detecting doublets by doublet simulation:&lt;/h3&gt;
&lt;p&gt;This method is quite simple. Select random pairs of cell barcodes from
an experiment. Then generate an artificial profile from those cell
barcodes by combining the counts from one cell barcode and the count 
from the other. (Scrublet allows for subsampling of the UMIs from the
artificially generated doublets, but by default there is no UMI
subsampling. UMI subsampling may be useful since creating artificial
doublets without subsampling could introduce bias towards high UMI cell
barcodes.) This new profile represents a single simulated doublet. This
process is repeated enough times to span the possible combinations of
cell doublets. For example, if there are 5 clusters representing 5 unique
cell types, it would be important to generate artificial doublets
arising from each possible cluster pairing. That way, the artificial
doublets will iterate all possible combinations of doublets arising from
droplets with different cell types. Once artificial doublets are
generated the next step is to identify the cell barcodes which are most
similar to the artificial profiles. When viewing these doublets in a
low-dimensional projection, they usually appear as an intermediate
cluster between two clusters.&lt;/p&gt;

&lt;h3 id=&quot;detecting-doublets-by-tcr-and-bcr-sequence-presence&quot;&gt;Detecting doublets by TCR and BCR sequence presence:&lt;/h3&gt;
&lt;p&gt;This method assumes that the presence of a TCR and BCR sequence for a
given barcode is not biologically reasonable. Thus, if a TCR and BCR
library identify sequences each with the same barcode, then it indicates
something went wrong and such barcodes should be removed. These barcodes
are the barcodes which are found in the BCR filtered contigs and the
TCR filtered contigs.&lt;/p&gt;

&lt;h3 id=&quot;dataset-and-results&quot;&gt;Dataset and results&lt;/h3&gt;
&lt;p&gt;Using &lt;a href=&quot;https://support.10xgenomics.com/single-cell-vdj/datasets/4.0.0/sc5p_v2_hs_melanoma_10k?&quot;&gt;this&lt;/a&gt; 10x Genomics melanoma public dataset I ran Scrublet using
standard parameters and a short script to detect doublets using the
validation method described above. The scrublet method detected 160
doublets while the validation method detected 168 doublets. The
intersection of these sets of barcodes was 117. I found this to be much
higher than I had expected, and I suppose that’s a good thing.&lt;/p&gt;

&lt;p&gt;The image below reiterates the overlap between the two methods of doublet
detection. There seems to be one main region in the top left (cloud of red)
which both methods classify as full of doublets. Around UMAP 1 coordinate 7 and 
UMAP 2 coordinate 2 there are a couple small clusters with a few cell
barcodes sprinkled in between. Scrublet labels some of them as doublets
but the validation method does not. I think this brings up an important 
limitation in the validation method. Because doublets are detected based
on the presence of a TCR and BCR contig, only T cell and B cell can be
detected. This also raises important questions about what the expected 
upper limit for overlap between the two methods putative doublets is,
since it must be below one. According to &lt;a href=&quot;https://www.biolegend.com/en-us/blog/expected-cell-frequencies&quot;&gt;Biolegend&lt;/a&gt;, the expected cell frequencies for PBMCs are
60-80% T cells and 5-15% B cells with natural killer cells, monocytes,
and dendritic cells making up the remaining population. Given melanoma
datasets are less common than PBMCs and possibly more variable, I’d like
to redo this analysis on a more common sample such as PBMCs. This would
allow further confidence in validation because when the underlying cell
frequencies are known, the hypothetical upper bound on putative doublet 
across methods can be better compared.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/scrublet-vs-tcrbcr-doublets.png&quot; alt=&quot;doublets&quot; /&gt;&lt;/p&gt;</content><author><name>Austin Hartman</name></author><summary type="html">In this post I’m going to briefly explore the validity of a specific type of doublet detection method, by assessing the validity of putative doublets by a mode not explored in the method’s paper. The method of doublet detection I am curious about are doublets detected based on similarity to artificially generated doublets. Multiple variants of this method have been published. For now, I am going to focus on one method called Scrublet. (Scrublet, DoubletFinder)</summary></entry><entry><title type="html">More on the hot hand</title><link href="http://localhost:4000/2018/06/18/more-on-the-hot-hand.html" rel="alternate" type="text/html" title="More on the hot hand" /><published>2018-06-18T00:00:00-07:00</published><updated>2018-06-18T00:00:00-07:00</updated><id>http://localhost:4000/2018/06/18/more-on-the-hot-hand</id><content type="html" xml:base="http://localhost:4000/2018/06/18/more-on-the-hot-hand.html">&lt;p&gt;In the initial article for this project I tried to find any evidence for 
or against the hot hand fallacy by analyzing free throws. Those findings 
indicated some evidence for the hot hand. The analysis showed that after 
making the first free throw, the following free throw had a 
significantly better than average chance of going in. The first article 
can be read &lt;a href=&quot;http://www.auhar.com/2018/05/31/free-throws-and-randomness.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After the free throw analysis I have decided to consider an analysis of 
in game shots.&lt;/p&gt;

&lt;p&gt;The first approach I thought of was very simple. I wanted to see how a players shooting 
percentage composed of shots after a make differed from their overall 
shooting percentage (similar to FT approach). Essentially, if a player makes a shot, could the 
player now ‘be on a roll.’ Of course, not every time a player makes a 
shot would fans consider them to be on fire, but this approach still 
seems that it could detect hot streaks if they really do exist. However, 
I also have to consider how a players’ approach could change after making 
a shot. It is certainly possible that after a player makes a shot, their 
heightened confidence would lead to them taking more difficult shots 
that they otherwise would be less likely to take. So this makes 
calculations a whole lot more difficult. With free throws, I could just 
look at shooting percentages before and after a make because the 
distance and the defence (because there is none) for the shot remain constant. If 
the shooting percentage after a make is higher, that gives a pretty good 
indication there is some sort of effect going on. However, with in game 
shots it is possible that a player has a higher shot making ability 
after making a shot, but his shooting percentage after a make is lower 
than his overall shooting percentage because the shots he attempted 
after the make were more difficult. This is why free throws were nice.&lt;/p&gt;

&lt;p&gt;So, to detect the hot hand for in game shots,  I will have to determine 
the difficulty of each shot taken. If the player performs better than 
the predicted difficulty after a make than when he typically shoots, 
then that would be very convincing evidence for the hot hand. The thing 
is, the only attribute in my dataset that would help indicate the shot 
difficulty is the distance of the shot. Typically, a longer shot is more 
difficult. To demonstrate this, I graphed the shooting percentages of 
all the shots in an NBA season from different distances below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/13-14.png&quot; alt=&quot;stock&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As expected, there is a downward trend as the distance increases. To my 
surprise, shooting percentage essentially plateaus at 40% from 5 to 20 
feet. If I had more data I might be able to decide why that is. Perhaps 
defenders defend more closely at 5 feet than they do at 20 feet which 
balances out the difficulty due to higher distance. I also created 
graphs for many other seasons as well, and the shape of the graph was 
approximately the same each season. The other charts can be found 
&lt;a href=&quot;https://github.com/AustinHartman/stats_scraper/tree/master/charts/shooting%20percentage%20vs%20dist&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So at the very least I could test this as an indicator of shot 
difficulty. Unfortunately, the lack of other information really makes it 
difficult to estimate the difficulty of a given shot. It would be 
helpful to know where the nearest defender was when the shot was taken. 
Was the shooter fully contested or wide open? It’d also be helpful to 
know the time on the shot clock. It is common to see a player throw up 
a prayer in the last few seconds of the shot clock, so knowing a shot 
was taken at the end of the shot clock often means it was a shot with a 
very low chance of going in. Obviously, there are plenty of factors that 
determine the difficulty of a shot but some of the factors are difficult 
to quantify and/or I have not found them.&lt;/p&gt;

&lt;p&gt;Because I do not have the detailed data I feel would be necessary to 
find significant statistical evidence for or against the hot hand, I 
will perform an analysis similar to what I did with free throws. 
However, this time I am going to do the analysis on a player by player 
basis. The reason for this is I only want to choose players who are 
taking shots regularly throughout the game. In other words, I would 
rather do this analysis on Lebron James than a role player like Cedi 
Osman. If a player shoots once in the first quarter and makes it and 
then takes his second shot of the game in the third quarter and makes 
it, nobody will assume his ability to make the shot was enhanced by his 
hot hand due to his shot made over an hour earlier. The idea is that the 
hot hand is temporary and that is why I want to only analyze players who 
will regularly be attempting shots.&lt;/p&gt;

&lt;p&gt;Below I graphed residuals of many top offensive players during the 
2014-2015 NBA season (I chose this season simply because this was the 
data I had available). Each residual point on the graph represents the 
difference between overall shooting percentage and shooting percentage when the 
previous shot was made.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/residuals.png&quot; alt=&quot;stock&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The graph shows that top players typically shoot a lower percentage 
after a make. The best explanation I have for this is top players 
attempt more ambitious shots after making a shot. So in the end, based 
on this data, I do not have enough evidence to comment specifically on 
the hot hand fallacy. However, I can say that many high volume shooters 
see a drop in shooting percentage after a make, but that does not mean 
their shot making ability drops after a make. Most 
likely this drop is due to players attempting more difficult shots after 
a make.&lt;/p&gt;</content><author><name>Austin Hartman</name></author><summary type="html">In the initial article for this project I tried to find any evidence for or against the hot hand fallacy by analyzing free throws. Those findings indicated some evidence for the hot hand. The analysis showed that after making the first free throw, the following free throw had a significantly better than average chance of going in. The first article can be read here.</summary></entry><entry><title type="html">basketball free throws, randomness, and the hot hand</title><link href="http://localhost:4000/2018/05/31/free-throws-and-randomness.html" rel="alternate" type="text/html" title="basketball free throws, randomness, and the hot hand" /><published>2018-05-31T00:00:00-07:00</published><updated>2018-05-31T00:00:00-07:00</updated><id>http://localhost:4000/2018/05/31/free-throws-and-randomness</id><content type="html" xml:base="http://localhost:4000/2018/05/31/free-throws-and-randomness.html">&lt;p&gt;For most sports fans the idea that a player is ‘on a roll’, or is ‘due 
to hit a shot’ is almost second nature. However, according to the 
statistics, sports contain much more randomness than most sports fans 
would be comfortable with. A paper published in &lt;a href=&quot;https://en.wikipedia.org/wiki/Hot-hand_fallacy#1985_%22Hot_Hand_in_Basketball%22_paper&quot;&gt;1985&lt;/a&gt; claimed the idea a 
player has a hot hand is simply a fallacy. The mind misinterprets these
occurences because our mind is trained to find patterns. When watching sports or going through life, 
it is common for to come up with stories to explain events that are 
inherently random. Claiming that a player ‘is on fire’ seems to just be 
another example of this. Our minds are biased towards looking for 
streaks, and random events can very often appear to show a streak.&lt;/p&gt;

&lt;p&gt;Let me propose an example. Below is the chart of a hypothetical stock coming
from a program randomly simulating stocks. This picture comes from a simulation 
where the stock begins the year at a value of $100 and each day its value 
will go up or down randomly (it is just as likely to go up as it is 
down) between 0 and 2 dollars. Of course some simulations of this will 
show shocks that shoot down, most will stay near $100 but some can seem 
to follow a steep upward trend. Is there any information that one could 
gain from this chart to make an informed decision about whether or not 
to purchase this stock? Absolutely not. The chart is completely 
misleading. This stock is no more likely to increase the following year than another 
simulation under the same conditions which ended up at $50 after one 
year. This is an example of how a trend or a perceived pattern seems 
to give a powerful indication of what will come in the future, and 
oftentimes that judgement is completely anchored in randomness. 
According to the hot hand fallacy, basketball is no exception.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/randstock.png&quot; alt=&quot;stock&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Of course after I say all of
these things about randomness, I am going to admit I wasn’t convinced by
the hot hand fallacy. Maybe part of me did not want to 
accept that all of the sports I’ve watched over the years were just a 
mess of randomness, but I wanted to see if I could find any of my own 
evidence for the existence of the hot hand. Sure, maybe sports fans 
blow the idea of the hot hand out of proportion. I could believe that. 
But to not exist at all, no way. I’ve played basketball. I know that 
feeling you get when you’ve hit a couple shots in a row. I thought that 
mental boost, the confidence, whatever it may be, must put you in some 
state of enhanced shot making ability.&lt;/p&gt;

&lt;p&gt;I thought about where to begin and I decided I would settle on an 
analysis of free throw shooting. I decided this is where I would start 
because of the simplicity of the free throw versus the complexity of 
analyzing in game shots. My thinking was a free throw is always taken 
from the same distance with no defence so any given free throw is just about 
the same as any other free throw. The reason this matters is because 
say we have a player who has a field goal percentage of 45%. Say he 
takes 10 shots in a game. Maybe three are layups, one dunk, two three 
pointers, three mid range jump shots, and a half court heave at the end 
of the half. Clearly, each of those shots have different likelihoods of 
going in. There is no way both the dunk and the half court heave have
exactly the same 45% chance of going in or even remotely similar chances
at that. It would be a lot harder to know if a player 
is shooting a better percentage when the degree of difficulty (which changes 
based on shot distance, defense, balance of the shooter etc.) of the 
players’ shots are never 
constant. But with free throws, I feel confident in assuming that the 
probability of a free throw going in is always just about what that 
players free throw percentage is. Of course, there are many factors 
that could change the percentage such as injury, fans in the background, 
fatigue, or even if the previous shot was made (if the hot hand fallacy 
is not actually a fallacy). Though these factors do (maybe) matter, they 
seem minor compared to the myriad of other factors that impact a players 
in game shooting.&lt;/p&gt;

&lt;p&gt;For my initial analysis, I found a nice dataset on a website called 
&lt;a href=&quot;https://www.kaggle.com/&quot;&gt;kaggle&lt;/a&gt;. Basically kaggle is a website full of freely 
accessible datasets. The dataset I used contained thousands 
and thousands of free throws taken between the 2006 and 2016 NBA season. 
My approach was simple. First I would calculate the average free throw 
percentage in the dataset. Then I would go through each set of two 
free throws and if the first free throw went in, then I would calculate 
the probability of the second going in. If the first free throw did not go in, 
then I’d instead use the data point to calculate the likelihood of the 
second free throw going in. What I hoped is that the free throw 
percentage would be above the average if the first one went in, and 
lower if the first one did not go in.&lt;/p&gt;

&lt;p&gt;Here are my findings:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;FT type&lt;/th&gt;
      &lt;th&gt;Make percentage&lt;/th&gt;
      &lt;th&gt;Sample size (shots)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Avg. FT %&lt;/td&gt;
      &lt;td&gt;75.7%&lt;/td&gt;
      &lt;td&gt;618,019&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;FT %: shot after make&lt;/td&gt;
      &lt;td&gt;79.6%&lt;/td&gt;
      &lt;td&gt;205,078&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;FT %: shot after miss&lt;/td&gt;
      &lt;td&gt;73.3%&lt;/td&gt;
      &lt;td&gt;72,961&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Clearly, there is some difference here. I know I should check P-values to 
verify that these differences are not random but because of the large
sample sizes I’m confident these results are not random when tested against a reasonable significance level. 
However, that  does not necessarily say much
about the validity of the hot hand. When fans mention the hot hand,
it usually does not have anything to do with free throws. At the very least,
I believe that the findings here show that even free throws, which have 
very few factors influencing the probability the shot goes in, do not have a constant 
make percentage. This variation is very promising because the 
hot hand fallacy assumes there should not be variance due to the 
state of the player (‘hot’ or ‘cold’). In further conclusion, it should be noted
that while there are differences in each free throw percentage, the differences are relatively small. 
They are not large enough for a fan to notice these effects during a game. 
Still, the effects certainly matter and are very encouraging.&lt;/p&gt;

&lt;p&gt;I’ll soon write a second piece using additional data from in game shots 
and I’ll see if I can back up my findings here.&lt;/p&gt;

&lt;p&gt;If you have any questions, comments, or ideas about where to go next,
please email me at the address found at the bottom of this site.&lt;/p&gt;</content><author><name>Austin Hartman</name></author><summary type="html">For most sports fans the idea that a player is ‘on a roll’, or is ‘due to hit a shot’ is almost second nature. However, according to the statistics, sports contain much more randomness than most sports fans would be comfortable with. A paper published in 1985 claimed the idea a player has a hot hand is simply a fallacy. The mind misinterprets these occurences because our mind is trained to find patterns. When watching sports or going through life, it is common for to come up with stories to explain events that are inherently random. Claiming that a player ‘is on fire’ seems to just be another example of this. Our minds are biased towards looking for streaks, and random events can very often appear to show a streak.</summary></entry></feed>